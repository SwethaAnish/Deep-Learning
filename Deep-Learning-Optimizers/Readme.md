## DEEP LEARNING OPTIMIZERS - IMPORTANT LINKS FOR REFERENCE 
### STOCHASTIC GRADIENT DESCENT : 
1. https://www.youtube.com/watch?v=FpDsDn-fBKA
2. https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/#:~:text=Stochastic%20Gradient%20Descent%20(SGD)%3A,data%20set%20for%20each%20iteration.

### MOMENTUM BASED STOCHASTIC GRADIENT DESCENT:
1. https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12
2. https://www.youtube.com/watch?v=CKLwvuKWQjo

### NESTEROV ACCELERATED GRADIENT DESCENT: 
1. https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12
2. https://www.youtube.com/watch?v=uHOTRHqnakQ

### ADAPTIVE GRADIENT DESCENT( Adagrad):
1. https://medium.com/konvergen/an-introduction-to-adagrad-f130ae871827
2. https://towardsdatascience.com/learning-parameters-part-5-65a2f3583f7d
3. https://www.geeksforgeeks.org/intuition-behind-adagrad-optimizer/

### RMS Prop:
1. https://towardsdatascience.com/learning-parameters-part-5-65a2f3583f7d


### ADAM:
1. https://towardsdatascience.com/learning-parameters-part-5-65a2f3583f7d